# CSE528_Final_Project_Associative_Memory_Neural_Networks
In this final project, two others and I characterized the differences in efficiency between simple RNN, LSTM, and GRU neural networks in accurately predicting words given a sequence of preceding letters. We determined that GRU neural network generalizes better than the simple RNN and LSTM neural networks in predicting different target sequences. Moreover, the GRU neural network required less training epochs for predicting the sequence "hello world" perfectly as compared to the simple RNN and LSTM neural networks. However, the simple RNN and LSTM neural networks could predict the target sequence perfectly with fewer recurrent units than the GRU neural network. Overall, these results may provide insights into how the nervous system optimally encodes sequences of items.  
